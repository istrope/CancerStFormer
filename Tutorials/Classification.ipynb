{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "300f96f9",
   "metadata": {},
   "source": [
    "# Pretrain and Create Model for Classification Based Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2580ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stFormer.classifier.Classifier import GenericClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b1ec1f",
   "metadata": {},
   "source": [
    "## 1.1 Classify From Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d4f877",
   "metadata": {},
   "source": [
    "We take out Subtype based information to evaluate classification fine-tuning and evaluation\n",
    "\n",
    "1. **Data Loading & Splitting**  \n",
    "   - Load `train_ds` from `dataset_path`.  \n",
    "   - If `eval_dataset_path` provided, load `eval_ds`;  \n",
    "     otherwise do a `train_test_split(test_size, seed=42)`.\n",
    "\n",
    "2. **`model_init` Function**  \n",
    "   - Loads base model & config from `model_checkpoint`.  \n",
    "   - Overrides `num_labels` to match `self.label_mapping`.  \n",
    "   - Optionally freezes the first `self.freeze_layers` encoder layers.\n",
    "   - Adds a classification head onto BERT pretreained model if loading from masked learning objective\n",
    "\n",
    "3. **Tokenizer & Data Collator**  \n",
    "   - `AutoTokenizer.from_pretrained(...)` with `padding=\"max_length\"`  \n",
    "   - `DataCollatorWithPadding` to pad to `tokenizer.model_max_length`.\n",
    "\n",
    "4. **Classification**\n",
    "    - `Evaluation metrics` compute metrics to determine training/test loss and accuracy\n",
    "    - `training args` takes dictionary of BERT training arguments for hyperparameter selection and model updating\n",
    "\n",
    "5.  **Best Checkpoint Selection and Saving**\n",
    "    - Saves model checkpoints to output directory based upon ``eval strategy` \n",
    "    - Returns final `trainer` model and saves final model to `output_directory`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1508f6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = GenericClassifier(\n",
    "    metadata_column = 'subtype',\n",
    "    nproc=24)\n",
    "    \n",
    "ds_path, map_path = classifier.prepare_data(\n",
    "    input_data_file = 'output/spot/visium_spot.dataset',\n",
    "    output_directory = 'tmp',\n",
    "    output_prefix = 'visium_spot'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4c9e22",
   "metadata": {},
   "source": [
    "In this example we utilize the model that was trained with a masked learning objective. While this is definitely possible, we suggest utilizing another Bert model that was trained using a classification task and then fine-tune on specific task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07976680",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = classifier.train(\n",
    "    model_checkpoint='output/spot/models/250422_102707_stFormer_L6_E3/final', # pretrained model path\n",
    "    dataset_path = ds_path, # dataset path from prepare data\n",
    "    output_directory = 'output/models/classification', #output evaluation \n",
    "    test_size=0.2, # splits dataset into test/train splits\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af33fda",
   "metadata": {},
   "source": [
    "## 1.2 Train Gene Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa7b7680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stFormer.classifier.Classifier import GenericClassifier\n",
    "import pandas as pd\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c9cb72",
   "metadata": {},
   "source": [
    "I am loading in the hallmark gene sets from msigDB to train the gene classifier in this example:\n",
    "1. load mapping dictionary to convert gene_name to ensembl ID\n",
    "2. load HALLMARK file\n",
    "3. create hallmark dictionary to term:genes for each hallmark gene set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da264104",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gene_name_id_dict_gc95M.pkl','rb') as f:\n",
    "    mapping_gene_dict = pickle.load(f)\n",
    "    \n",
    "gmt_file = pd.read_csv('data/h.all.v2024.1.Hs.symbols.gmt',sep='\\t',index_col=None,header=None)\n",
    "hallmark = {}\n",
    "for index,row in gmt_file.iterrows():\n",
    "    term = row[0]\n",
    "    genes = [x for x in row[1:] if pd.notna(x)] #extract genes\n",
    "    ensembl_genes = [mapping_gene_dict[x] for x in genes if x in mapping_gene_dict] #convert to ensembl based on dictionary\n",
    "    hallmark[term] = ensembl_genes #make dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffd523a",
   "metadata": {},
   "source": [
    "### GeneClassifier Token Classification Overview\n",
    "\n",
    "We take out per-cell Subtype labels and instead classify **individual genes (tokens)** within each sequence.\n",
    "\n",
    "1. **Data Loading & Splitting**  \n",
    "   - Load `train_ds` from `dataset_path`.  \n",
    "   - If `eval_dataset_path` is provided, load `eval_ds`.  \n",
    "     Otherwise, perform `train_test_split(test_size, seed=42)`.\n",
    "\n",
    "2. **Label Mapping (Gene Classes)**  \n",
    "   - Use `classifier_utils.label_classes(\"gene\", ...)` to map each **input token (gene)** to a class label.  \n",
    "   - Generates a per-token `labels` field matching `input_ids` shape.\n",
    "\n",
    "3. **Tokenizer & Data Collator**   \n",
    "   - Uses `DataCollatorForGeneClassification` to pad both `input_ids` and `labels` in sync.\n",
    "\n",
    "4. **Model Initialization**  \n",
    "   - Loads base model & config from `model_checkpoint`.  \n",
    "   - Creates a `TokenClassification` head on the pretrained model.\n",
    "\n",
    "5. **Classification Training**  \n",
    "   - Computes token-level metrics (e.g., F1 score, accuracy).  \n",
    "\n",
    "6. **Best Checkpoint Selection and Saving**  \n",
    "   - Saves model checkpoints to output directory based on `evaluation_strategy`.  \n",
    "   - Final model and tokenizer are saved to `output_directory`.  \n",
    "   - Predictions and evaluation metrics are returned for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6760dd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_class_dict = hallmark\n",
    "\n",
    "# 2) Instantiate for token-classification\n",
    "gene_classifier = GenericClassifier(\n",
    "    metadata_column=None,             # no sequence-level label\n",
    "    gene_class_dict=gene_class_dict, #specify this dictionary to use Gene Classifier\n",
    "    classifier_type = 'token',\n",
    "    freeze_layers=2,                  # freeze first two BERT layers (optional)\n",
    "    forward_batch_size=64,\n",
    "    nproc=16,\n",
    "    token_dictionary_file='output_test/token_dictionary.pickle'\n",
    ")\n",
    "\n",
    "# 3) Prepare your dataset (must already contain `input_ids` for each cell)\n",
    "ds_path, map_path = gene_classifier.prepare_data(\n",
    "    input_data=\"output_test/spot/visium_spot.dataset\",\n",
    "    output_directory=\"tmp/gene_tokens\",\n",
    "    output_prefix=\"visium_gene\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d937989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# load in dataset and view the gene labels based upon mapping dictionary (evaluate to see if not all -100 (no map)) \n",
    "ds = load_from_disk(ds_path)\n",
    "np.unique(ds['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fafed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gene_classifier.train(\n",
    "    model_checkpoint=\"output_test/pretrained_model/models/250520_111122_STgeneformer_30M_L6_emb256_SL2048_E3_B24_LR0.001_LSlinear_WU10000_O0.001_DS/final\",\n",
    "    dataset_path=ds_path,\n",
    "    output_directory=\"output_test/models/visium_gene_classifier\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b1bf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = gene_classifier.evaluate(\n",
    "    model_directory=\"models/visium_gene_classifier/final_model\",\n",
    "    eval_dataset_path=ds_path,\n",
    "    id_class_dict_file=map_path,\n",
    "    output_directory=\"models/visium_gene_classifier\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d518c2b0",
   "metadata": {},
   "source": [
    "## 1.3 Train and Evaluate Model with Hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b835a3",
   "metadata": {},
   "source": [
    "In this example we utilize ray configuration to loop through a list of hyperparameters to search for the best configuration of arguments for a classification task.\n",
    "\n",
    "\n",
    "Performs end-to-end hyperparameter search for a sequence-classification head using Ray Tune and Hugging Face Trainer.\n",
    "1. **Define Hyperparameter Search Space**  \n",
    "   - Pull ranges/choices from `self.ray_config` for  \n",
    "     `learning_rate`, `num_train_epochs`, `weight_decay`, etc.  \n",
    "\n",
    "2. **CLI Reporter**  \n",
    "   - `CLIReporter` shows per-trial metrics (`eval_loss`, `eval_accuracy`)  \n",
    "     and hyperparameter values in the console.\n",
    "\n",
    "3. **Trainer & Hyperparameter Search**  \n",
    "   - Instantiate `Trainer` with `model_init`, datasets, collator, and `compute_metrics`.  \n",
    "   - Run `trainer.hyperparameter_search(...)` with Ray backend and `HyperOptSearch`.\n",
    "\n",
    "4. **Best Checkpoint Selection & Saving**  \n",
    "    - Use `ExperimentAnalysis` to find best trial/checkpoint by `eval_loss`.  \n",
    "    - Load that checkpoint into a fresh `BertForSequenceClassification`.  \n",
    "    - Save model & tokenizer under `output_directory/best_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84363352",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = GenericClassifier(\n",
    "    metadata_column = 'subtype',\n",
    "    ray_config={\n",
    "        \"learning_rate\":[1e-5,5e-5], #loguniform learning rate\n",
    "        \"num_train_epochs\":[2,3], \n",
    "        \"weight_decay\": [0.0, 0.3], #tune.uniform across values\n",
    "        'lr_scheduler_type': [\"linear\",\"cosine\",\"polynomial\"], #scheduler\n",
    "        'seed':[0,100]\n",
    "        },\n",
    "    nproc = 24,\n",
    "    n_trials = 10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c26cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path, map_path = classifier.prepare_data(\n",
    "    input_data_file = 'output/spot/visium_spot.dataset',\n",
    "    output_directory = 'tmp',\n",
    "    output_prefix = 'visium_spot'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e4c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = classifier.train(\n",
    "    model_checkpoint='output/spot/models/250422_102707_stFormer_L6_E3/final',\n",
    "    dataset_path = ds_path,\n",
    "    output_directory = 'output/models/tuned_classification',\n",
    "    n_trials=10,\n",
    "    test_size=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bab0f33",
   "metadata": {},
   "source": [
    "## 1.3 Plot Predictions using Evaluation Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba65199",
   "metadata": {},
   "source": [
    "Utilize seaborn, truth, and predicted values to create a confusion matrix and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e3f11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stFormer.classifier.Classifier import GenericClassifier\n",
    "from datasets import load_from_disk\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "#Produce & save raw predictions\n",
    "eval_ds = load_from_disk(ds_path).shuffle(seed=42).select(range(1000))\n",
    "preds = trainer.predict(eval_ds)\n",
    "y_true = preds.label_ids\n",
    "y_pred = preds.predictions.argmax(-1)\n",
    "\n",
    "with open(\"output/models/classification/predictions.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"y_true\": y_true, \"y_pred\": y_pred}, f)\n",
    "\n",
    "\n",
    "# Load the id→class mapping you dumped in prepare_data()\n",
    "with open(map_path, \"rb\") as f:\n",
    "    id_map = pickle.load(f)       \n",
    "\n",
    "# We need a list of class names in label‐index order:\n",
    "inv_map = {v:k for k,v in id_map.items()}\n",
    "class_order = [inv_map[i] for i in range(len(inv_map))]\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(id_map.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7a6169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=class_order, yticklabels=class_order, cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bc9ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save heatmap with inbuilt plotting functionality\n",
    "classifier.plot_predictions(\n",
    "    predictions_file=\"output/models/classification/predictions.pkl\",\n",
    "    id_class_dict_file=map_path,\n",
    "    title=\"Visium Spot Subtype Predictions\",\n",
    "    output_directory=\"output/models/classification\",\n",
    "    output_prefix=\"visium_spot\",\n",
    "    class_order=class_order\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stFormer",
   "language": "python",
   "name": "stformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

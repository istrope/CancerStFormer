{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "300f96f9",
   "metadata": {},
   "source": [
    "# Pretrain and Create Model for Classification Based Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2580ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stFormer.classifier.Classifier import GenericClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b1ec1f",
   "metadata": {},
   "source": [
    "## 1.1 Classify From Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d4f877",
   "metadata": {},
   "source": [
    "We take out Subtype based information to evaluate classification fine-tuning and evaluation\n",
    "\n",
    "1. **Data Loading & Splitting**  \n",
    "   - Load `train_ds` from `dataset_path`.  \n",
    "   - If `eval_dataset_path` provided, load `eval_ds`;  \n",
    "     otherwise do a `train_test_split(test_size, seed=42)`.\n",
    "\n",
    "2. **`model_init` Function**  \n",
    "   - Loads base model & config from `model_checkpoint`.  \n",
    "   - Overrides `num_labels` to match `self.label_mapping`.  \n",
    "   - Optionally freezes the first `self.freeze_layers` encoder layers.\n",
    "   - Adds a classification head onto BERT pretreained model if loading from masked learning objective\n",
    "\n",
    "3. **Tokenizer & Data Collator**  \n",
    "   - `AutoTokenizer.from_pretrained(...)` with `padding=\"max_length\"`  \n",
    "   - `DataCollatorWithPadding` to pad to `tokenizer.model_max_length`.\n",
    "\n",
    "4. **Classification**\n",
    "    - `Evaluation metrics` compute metrics to determine training/test loss and accuracy\n",
    "    - `training args` takes dictionary of BERT training arguments for hyperparameter selection and model updating\n",
    "\n",
    "5.  **Best Checkpoint Selection and Saving**\n",
    "    - Saves model checkpoints to output directory based upon ``eval strategy` \n",
    "    - Returns final `trainer` model and saves final model to `output_directory`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1508f6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = GenericClassifier(\n",
    "    metadata_column = 'subtype',\n",
    "    nproc=24)\n",
    "    \n",
    "ds_path, map_path = classifier.prepare_data(\n",
    "    input_data_file = 'output/spot/visium_spot.dataset',\n",
    "    output_directory = 'tmp',\n",
    "    output_prefix = 'visium_spot'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4c9e22",
   "metadata": {},
   "source": [
    "In this example we utilize the model that was trained with a masked learning objective. While this is definitely possible, we suggest utilizing another Bert model that was trained using a classification task and then fine-tune on specific task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07976680",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = classifier.train(\n",
    "    model_checkpoint='output/spot/models/250422_102707_stFormer_L6_E3/final', # pretrained model path\n",
    "    dataset_path = ds_path, # dataset path from prepare data\n",
    "    output_directory = 'output/models/classification', #output evaluation \n",
    "    test_size=0.2, # splits dataset into test/train splits\n",
    "    evaluation_dataset = None # set path to outside dataset for external validation instead of test/train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d518c2b0",
   "metadata": {},
   "source": [
    "## 1.2 Train and Evaluate Model with Hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b835a3",
   "metadata": {},
   "source": [
    "In this example we utilize ray configuration to loop through a list of hyperparameters to search for the best configuration of arguments for a classification task.\n",
    "\n",
    "\n",
    "Performs end-to-end hyperparameter search for a sequence-classification head using Ray Tune and Hugging Face Trainer.\n",
    "1. **Define Hyperparameter Search Space**  \n",
    "   - Pull ranges/choices from `self.ray_config` for  \n",
    "     `learning_rate`, `num_train_epochs`, `weight_decay`, etc.  \n",
    "\n",
    "2. **CLI Reporter**  \n",
    "   - `CLIReporter` shows per-trial metrics (`eval_loss`, `eval_accuracy`)  \n",
    "     and hyperparameter values in the console.\n",
    "\n",
    "3. **Trainer & Hyperparameter Search**  \n",
    "   - Instantiate `Trainer` with `model_init`, datasets, collator, and `compute_metrics`.  \n",
    "   - Run `trainer.hyperparameter_search(...)` with Ray backend and `HyperOptSearch`.\n",
    "\n",
    "4. **Best Checkpoint Selection & Saving**  \n",
    "    - Use `ExperimentAnalysis` to find best trial/checkpoint by `eval_loss`.  \n",
    "    - Load that checkpoint into a fresh `BertForSequenceClassification`.  \n",
    "    - Save model & tokenizer under `output_directory/best_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84363352",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = GenericClassifier(\n",
    "    metadata_column = 'subtype',\n",
    "    ray_config={\n",
    "        \"learning_rate\":[1e-5,5e-5], #loguniform learning rate\n",
    "        \"num_train_epochs\":[2,3], \n",
    "        \"weight_decay\": [0.0, 0.3], #tune.uniform across values\n",
    "        'lr_scheduler_type': [\"linear\",\"cosine\",\"polynomial\"], #scheduler\n",
    "        'seed':[0,100]\n",
    "        },\n",
    "    nproc = 24\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c26cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path, map_path = classifier.prepare_data(\n",
    "    input_data_file = 'output/spot/visium_spot.dataset',\n",
    "    output_directory = 'tmp',\n",
    "    output_prefix = 'visium_spot'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e4c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = classifier.train(\n",
    "    model_checkpoint='output/spot/models/250422_102707_stFormer_L6_E3/final',\n",
    "    dataset_path = ds_path,\n",
    "    output_directory = 'output/models/tuned_classification',\n",
    "    n_trials=10,\n",
    "    test_size=0.2,\n",
    "    #stratify=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bab0f33",
   "metadata": {},
   "source": [
    "## 1.3 Plot Predictions using Evaluation Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba65199",
   "metadata": {},
   "source": [
    "Utilize seaborn, truth, and predicted values to create a confusion matrix and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e3f11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stFormer.classifier.Classifier import GenericClassifier\n",
    "from datasets import load_from_disk\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "#Produce & save raw predictions\n",
    "eval_ds = load_from_disk(ds_path).shuffle(seed=42).select(range(1000))\n",
    "preds = trainer.predict(eval_ds)\n",
    "y_true = preds.label_ids\n",
    "y_pred = preds.predictions.argmax(-1)\n",
    "\n",
    "with open(\"output/models/classification/predictions.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"y_true\": y_true, \"y_pred\": y_pred}, f)\n",
    "\n",
    "\n",
    "# Load the id→class mapping you dumped in prepare_data()\n",
    "with open(map_path, \"rb\") as f:\n",
    "    id_map = pickle.load(f)       \n",
    "\n",
    "# We need a list of class names in label‐index order:\n",
    "inv_map = {v:k for k,v in id_map.items()}\n",
    "class_order = [inv_map[i] for i in range(len(inv_map))]\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(id_map.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7a6169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=class_order, yticklabels=class_order, cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bc9ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save heatmap with inbuilt plotting functionality\n",
    "classifier.plot_predictions(\n",
    "    predictions_file=\"output/models/classification/predictions.pkl\",\n",
    "    id_class_dict_file=map_path,\n",
    "    title=\"Visium Spot Subtype Predictions\",\n",
    "    output_directory=\"output/models/classification\",\n",
    "    output_prefix=\"visium_spot\",\n",
    "    class_order=class_order\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

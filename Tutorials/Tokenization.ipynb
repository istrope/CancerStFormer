{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93164d94",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d5ae215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stFormer.tokenization.median_estimator import MedianEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fec4d1e",
   "metadata": {},
   "source": [
    "## 1.1 Create Gene Medians and T-Digests\n",
    "1. Compute T-Digest: compact summary of full distribution of normalized expression data\n",
    "2. Compute Gene Medians (Measure of Central Tendency): Summary of typical expression for each gene\n",
    "3. Write files to output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8a43cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MedianEstimator Object\n",
    "estimator = MedianEstimator(\n",
    "    data_dir = 'data', # directory where anndata or loom files are housed for tokenization/model creation\n",
    "    extension = '.h5ad', # type of file Literal['.h5ad','.loom']\n",
    "    out_path = 'output', # output directory where files are written\n",
    "    merge_tdigests = True # option to merge multiple samples gene distributions (set False if only one dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614c107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.compute_tdigests() # worker for computing tdigests (merges distribuitons if more than one dataset)\n",
    "estimator.tdigests # show tdigests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d49af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_median_dict() # worker for summarizing gene expression \n",
    "estimator.median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11beb89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.write_tdigests() # write to out_path\n",
    "estimator.write_medians() # write to out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9c43e2",
   "metadata": {},
   "source": [
    "## 1.2 Tokenize Spot Level Resolution (55um)\n",
    "This option will take in each cells (spot) gene expression and tokenize dataset through \n",
    "1. Select non-zero genes\n",
    "2. normalize counts\n",
    "3. maps genes -> tokens\n",
    "4. create ranks for genes\n",
    "5. truncates sequence length (tokens) to set length (ex: 2048 tokens aka: \"genes\")\n",
    "6. Bundles into Hugging Face Dataset optionally augmented with cell metadata (e.g., sample_ID, coordinates, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24b78d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stFormer.tokenization.SpatialTokenize import SpatialTokenizer, create_token_dictionary\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d601966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create token dictionary\n",
    "token_dictionary = create_token_dictionary(median_dict=median_dict)\n",
    "with open('output/token_dictionary.pickle','wb') as file:\n",
    "    pickle.dump(token_dictionary,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db373dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_spot = SpatialTokenizer(\n",
    "    mode='spot', #mode is spot/resolution 55um\n",
    "    gene_length=2048, # truncate sequence length to 2048 tokens per cell\n",
    "    custom_meta = {'patient_id':'sample','classification':'classification','subtype':'subtype'},\n",
    "    nproc=16, #number of CPUs for data I/O\n",
    "    gene_median_file='output/gene_medians.pickle', # location to gene_medians\n",
    "    token_dict_file='output/token_dictionary.pickle', # location of token dictionary\n",
    "    )\n",
    "\n",
    "tok_spot.tokenize(\n",
    "    data_dir=Path('data'), # location of h5ad/loom data\n",
    "    out_dir=Path('output/spot'), # where to write tokenized data to\n",
    "    prefix='visium_spot', # prefix of files output\n",
    "    file_format='h5ad' # what the file format is Literal['h5ad','loom']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecbdce6",
   "metadata": {},
   "source": [
    "## 1.3 Tokenize Neighbor Level Resolution\n",
    "Performs the same functions as spot level resolution but with key changes to theoretically model both spot level expression as well as neighborhood level expression (consider this as the niche/neighborhood tokenization)\n",
    "1. Calculates gene ranks for spot\n",
    "2. Creates connectivities matrix to locate neighbors based upon coordinate information\n",
    "3. Calculates gene ranks for each neighboring spot and averages ranks\n",
    "4. Concatenates truncated tokens (spot) + truncated tokens (neighborhood)\n",
    "\n",
    "\n",
    "**Note:** gene_length = 2048 means that both spot and neighbor tokens are truncated to 2048, so the final embedding size should be 4097"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741cbab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_neighbor = SpatialTokenizer(\n",
    "    mode='neighborhood',\n",
    "    gene_length=2048,\n",
    "    custom_meta = {'patient_id':'sample','classification':'classification','subtype':'subtype'},\n",
    "    nproc=16,\n",
    "    gene_median_file='output/gene_medians.pickle',\n",
    "    token_dict_file='output/token_dictionary.pickle'\n",
    "    )\n",
    "\n",
    "tok_neighbor.tokenize(\n",
    "    data_dir=Path('data'),\n",
    "    out_dir=Path('output/neighborhood'),\n",
    "    prefix='visium_neighborhood',\n",
    "    file_format='h5ad')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7200c2a8",
   "metadata": {},
   "source": [
    "## 1.4 Visualize Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c10aaf",
   "metadata": {},
   "source": [
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29046fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dss = load_from_disk('output/spot/visium_spot.dataset')\n",
    "dsn = load_from_disk('output/neighborhood/visium_neighborhood.dataset')\n",
    "dfs = dss.to_pandas()\n",
    "dfn = dsn.to_pandas()\n",
    "dfn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stFormer",
   "language": "python",
   "name": "stformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
